# Batch Normalization (From Scratch)

Ce dossier contient une impl√©mentation simple de la technique de **Batch Normalization** en utilisant **PyTorch**, dans le cadre d‚Äôun framework de deep learning minimaliste d√©velopp√© from scratch.

## üìå Description

La **Batch Normalization** est une technique utilis√©e pour am√©liorer la stabilit√© et la vitesse d'entra√Ænement des r√©seaux de neurones profonds.  
Elle normalise les activations de chaque batch pour qu'elles aient une moyenne proche de 0 et une variance proche de 1, puis r√©applique deux param√®tres apprenables (`gamma` et `beta`) pour conserver la capacit√© de moduler la distribution des activations.

Cette impl√©mentation permet de :
- Calculer la moyenne et la variance d‚Äôun batch.
- Normaliser les valeurs.
- R√©appliquer une √©chelle (`gamma`) et un biais (`beta`).
- Retourner les activations normalis√©es.


